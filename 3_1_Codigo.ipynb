{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3.1.-Codigo.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aszapla/Curso-DL/blob/master/3_1_Codigo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRXW9rnD8km0",
        "colab_type": "text"
      },
      "source": [
        "# Sesión 3.1: Redes recurrentes avanzadas\n",
        "\n",
        "Profesor: [Jorge Calvo Zaragoza](mailto:jcalvo@prhlt.upv.es)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYGWCLFVuuRp",
        "colab_type": "text"
      },
      "source": [
        "## Ejemplo guiado: traducción neuronal\n",
        "\n",
        "\n",
        "\n",
        "En esta sesión veremos una arquitectura secuencia a secuencia (sequence-to-sequence, o *seq2seq*) para traducción neuronal de inglés a español. Implementaremos la red neuronal siguiente una arquitectura encoder-decoder. La estructura se ilustra en la siguiente imagen:\n",
        "\n",
        "![texto alternativo](https://docs.google.com/uc?id=189LP5y8Q2ocge971-nzUUXQb4yFzDdyr)\n",
        "\n",
        "\n",
        "Como se puede apreciar, vamos a realizar la traducción **a nivel de caracter**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my3iXcDpR9Nl",
        "colab_type": "text"
      },
      "source": [
        "## Preliminares: generadores\n",
        "\n",
        "En el caso de tareas relacionadas con redes recurrentes, especialmente cuando se plantean con el enfoque encoder-decoder, es usual tener entradas y salidas de longitud variable. A pesar de que, por definición, las redes recurrentes pueden manejar secuencias de tamaño variable, los paquetes de datos tienen que fijarse a un tipo de dimension concreta: internamente Keras (al igual que otros entornos) trabaja con tensores, los cuales deben tener definidas sus dimensiones. \n",
        "\n",
        "Para paliar este problema, existen dos alternativas: crear tantos paquetes como secuencias haya (lo cual es ineficiente en el entrenamiento) o utilizar la técnica de *padding*. El padding consiste en calcular la secuencia de mayor longitud y establecer las dimensiones acorde a este valor. Las secuencias más cortas son rellenadas con valores nulos (típicamente 0).\n",
        "\n",
        "No obstante, existe una solución intermedia, más elegante, para solventar esta cuestión, a traves de funciones *generadoras*. Las generadoras son funciones que preparan los paquetes de datos. En cada llamada, la generadora prepara el siguiente paquete a tener en cuenta y se *congela*, hasta que es llamado de nuevo para generar otro paquete. Además de la cuestión relacionada con los tensores, las generadoras son de gran utilidad cuando afrontamos una tarea de gran entidad, ya que podría ser inmanejable cargar todo el conjunto de entrenamiento en memoria. De esta forma, sólo se utiliza la memoria que ocupa un único paquete.\n",
        "\n",
        "En el siguiente código se proporciona un ejemplo intuitivo de este comportamiento (especial atención a las palabras clave **yield** y **next**)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-25-bLnCR8BY",
        "colab_type": "code",
        "outputId": "6204c762-16c8-4b80-e452-0dbadfe9735c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "\n",
        "def custom_generator(batch_size):\n",
        "    X = 'abcdefghij'\n",
        "    Y = '0123456789'\n",
        "    \n",
        "    while True:\n",
        "      for idx in range(0,len(X),batch_size):\n",
        "        yield X[idx:idx+batch_size], Y[idx:idx+batch_size]\n",
        "            \n",
        "          \n",
        "generator = custom_generator(batch_size = 4)\n",
        "\n",
        "for _ in range(5):          \n",
        "  x,y  = next(generator)\n",
        "  print(x)\n",
        "  print(y)\n",
        "  print('')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "abcd\n",
            "0123\n",
            "\n",
            "efgh\n",
            "4567\n",
            "\n",
            "ij\n",
            "89\n",
            "\n",
            "abcd\n",
            "0123\n",
            "\n",
            "efgh\n",
            "4567\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1k4h-AFY_vM",
        "colab_type": "text"
      },
      "source": [
        "## Código Keras paso a paso\n",
        "\n",
        "Para entrenar un traductor automático neuronal, sólo necesitamos frases en el idioma origen y su correspondiente traducción en el idioma destino. A diferencia de otro tipo de sistemas, no es necesario proporcionar un alineamiento entre los elementos de cada frase. \n",
        "\n",
        "Para este ejemplo vamos a utilizar el corpus que se proporciona en [este enlace](http://www.dlsi.ua.es/~jcalvo/data/eng-spa.txt). Este corpus se ha extraído de [este repositorio](http://www.manythings.org/anki/), con diversas opciones para otros pares de idiomas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5GB4PkniBl6",
        "colab_type": "code",
        "outputId": "6c2a958e-b116-49aa-df74-295601b8e0a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "import urllib.request # Python3\n",
        "\n",
        "# Número de pares de secuencias que vamos a considerar\n",
        "limit_of_sentences = 4000 \n",
        "\n",
        "# Se descarga el bitexto y se descompone en lineas\n",
        "bitext_url = \"http://www.dlsi.ua.es/~jcalvo/data/eng-spa.txt\" \n",
        "response = urllib.request.urlopen(bitext_url)\n",
        "bibtext = response.read().decode('utf-8').splitlines()\n",
        "\n",
        "# Creamos los corpus de idioma origen y destino\n",
        "data_input = []\n",
        "data_output = []\n",
        "\n",
        "for idx in range( min(len(bibtext), limit_of_sentences) ):\n",
        "    english, spanish = bibtext[idx].split('\\t')\n",
        "    data_input.append( english )\n",
        "    data_output.append( spanish )\n",
        "  \n",
        "# Comprobación\n",
        "print(data_input[4])\n",
        "print(data_output[4])\n",
        "print()\n",
        "print(data_input[-4])\n",
        "print(data_output[-4])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hi.\n",
            "Hola.\n",
            "\n",
            "He made me go.\n",
            "Él me hizo ir.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh9xPOaqkWIv",
        "colab_type": "text"
      },
      "source": [
        "Los datos vienen ordenados: la red podría utilizar el orden para *ahorrarse* información en el vector de contexto, lo cual podría ser perjudicial para el aprendizaje. Así pues, barajamos los datos manteniendo el alineamiento entre las frases:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0SKWTZ8khZF",
        "colab_type": "code",
        "outputId": "63c0baed-00a8-410e-d2a6-dafd9bfe54aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "import random\n",
        "\n",
        "# Enlazamos ambas listas \n",
        "bitext_sets = list(zip(data_input, data_output))\n",
        "\n",
        "# Se barajan y se desenlazan de nuevo\n",
        "random.shuffle(bitext_sets)\n",
        "data_input[:], data_output[:] = zip(*bitext_sets)\n",
        "\n",
        "# Comprobación\n",
        "print(data_input[4])\n",
        "print(data_output[4])\n",
        "print()\n",
        "print(data_input[-4])\n",
        "print(data_output[-4])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Has he come?\n",
            "¿Ha venido?\n",
            "\n",
            "Grab a seat.\n",
            "Toma un asiento.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWqxjoZHZJ9K",
        "colab_type": "text"
      },
      "source": [
        "En este punto vamos a incorporar los caracteres de comienzo de secuencia (start of sentence, *sos*) y final de secuencia (end of sentence, *eos*). Vamos a utilizar dos caracteres especiales para indicarlos, con atención a que éstos no pertenezcan a los alfabetos de salida.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFuhh12gZQo4",
        "colab_type": "code",
        "outputId": "6a698350-6468-4897-b43a-4304f2026f58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Caracteres de comienzo y final    \n",
        "output_sos = '<'\n",
        "output_eos = '>'\n",
        "\n",
        "# Las datos de salida se transforman para incluir ambos\n",
        "data_output = [output_sos + output_sentence + output_eos for output_sentence in data_output]\n",
        "\n",
        "# Comprobación\n",
        "print(data_output[4])\n",
        "print()\n",
        "print(data_output[-4])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<¿Ha venido?>\n",
            "\n",
            "<Toma un asiento.>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwx74kgoZ5Rx",
        "colab_type": "text"
      },
      "source": [
        "Ahora necesitamos conocer los alfabetos de entrada y salida, especialmente su longitud. Esta información es necesaria para saber la dimension de la representación one hot a utilizar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUuepHIiZ9WY",
        "colab_type": "code",
        "outputId": "a1fccdca-7296-42a9-f001-408040bbc0cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Conjuntos de caracteres\n",
        "input_alphabet = set()\n",
        "output_alphabet = set()\n",
        "\n",
        "for input_sentence in data_input:\n",
        "    input_alphabet.update(list(input_sentence))\n",
        "\n",
        "for output_sentence in data_output:\n",
        "    output_alphabet.update(list(output_sentence))\n",
        "\n",
        "# Guardamos sus longitudes en variables\n",
        "input_alphabet_len = len(input_alphabet)\n",
        "output_alphabet_len = len(output_alphabet)\n",
        "\n",
        "# Comprobación    \n",
        "print('Alfabeto de entrada de ' + str(input_alphabet_len) + ' caracteres: ' + str(input_alphabet))    \n",
        "print('Alfabeto de salida de ' + str(output_alphabet_len) + ' caracteres: ' + str(output_alphabet))    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Alfabeto de entrada de 65 caracteres: {'0', 'b', 'F', 'V', 'w', 'U', 'i', 'B', 'r', '.', 'Y', 'J', '$', 'v', 'A', 'L', 'O', 'l', \"'\", '7', 'D', 'g', '?', 'p', 'h', 'C', 'n', 'q', 'k', '1', 'e', 'R', '3', 'z', 'y', 'W', 'K', '8', 'T', ':', ',', 'o', 's', 'c', '5', '9', 'M', 'Q', '!', 'j', 'd', 'H', ' ', 'S', 'N', 't', 'E', 'G', 'm', 'f', 'x', 'u', 'I', 'a', 'P'}\n",
            "Alfabeto de salida de 77 caracteres: {'Á', '0', 'í', 'b', 'F', 'V', 'U', 'ó', 'i', 'B', 'r', '.', 'Y', 'J', 'Ó', 'v', 'Ú', 'L', 'A', 'O', 'l', 'é', '7', 'D', 'g', 'p', '?', 'É', 'h', 'C', '\"', 'n', 'q', 'k', '1', 'e', 'R', 'á', '3', '>', 'z', 'y', 'ú', '8', 'T', 'ñ', ',', '¡', '<', ':', 'o', 's', 'c', 'ü', '5', '«', 'M', 'Q', '!', 'j', 'd', 'H', ' ', 'S', 'N', 't', 'E', 'G', 'm', 'f', '»', 'x', 'u', 'I', 'a', '¿', 'P'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPB7eNRhaLt_",
        "colab_type": "text"
      },
      "source": [
        "Asignamos un valor numérico identificativo a cada caracter, guardando la conversión entre ambas representaciones para el futuro."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eV8vU7bEaWpR",
        "colab_type": "code",
        "outputId": "fcf8a4c0-1c15-4897-bfb5-d2c706757c77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# De caracter a entero\n",
        "input_alphabet_from_char_to_int = dict([(char, i) for i, char in enumerate(input_alphabet)])\n",
        "output_alphabet_from_char_to_int = dict([(char, i) for i, char in enumerate(output_alphabet)])    \n",
        "\n",
        "# De entero a caracter\n",
        "input_alphabet_from_int_to_char = dict([(i, char) for i, char in enumerate(input_alphabet)])\n",
        "output_alphabet_from_int_to_char = dict([(i, char) for i, char in enumerate(output_alphabet)])\n",
        "\n",
        "# Comprobación\n",
        "print('Identificador 0 para el vocabulario de entrada: ' + str(input_alphabet_from_int_to_char[0]))\n",
        "print('Identificador de \\'a\\' para el vocabulario de entrada: ' + str(input_alphabet_from_char_to_int['a']))\n",
        "print('')\n",
        "print('Identificador 0 para el vocabulario de salida: ' + str(output_alphabet_from_int_to_char[0]))\n",
        "print('Identificador de \\'a\\' para el vocabulario de salida: ' + str(output_alphabet_from_char_to_int['a']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Identificador 0 para el vocabulario de entrada: 0\n",
            "Identificador de 'a' para el vocabulario de entrada: 63\n",
            "\n",
            "Identificador 0 para el vocabulario de salida: Á\n",
            "Identificador de 'a' para el vocabulario de salida: 74\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp6pw64dePen",
        "colab_type": "text"
      },
      "source": [
        "Construimos los generadores que se usarán para alimentar a la red. Debe tenerse en cuenta que hay que hacer *padding* dentro de cada paquete creado, ya que los tensores a devolver deben tener una dimensión definida."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TP3RLK8UeUNn",
        "colab_type": "code",
        "outputId": "86210840-c96a-426b-b2ee-5ea5d6900ed4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "# Función auxiliar que convierte listas en paquetes de datos\n",
        "def batch_to_vectors(data_input, data_output):\n",
        "  max_input_len = max([len(input_sentence) for input_sentence in data_input])\n",
        "  max_output_len = max([len(output_sentence) for output_sentence in data_output])\n",
        "\n",
        "  encoder_input = np.zeros((len(data_input),max_input_len,input_alphabet_len), dtype=np.float)\n",
        "\n",
        "  for idx_s, input_sentence in enumerate(data_input):        \n",
        "      for idx_c, char in enumerate(input_sentence):\n",
        "          encoder_input[idx_s][idx_c][input_alphabet_from_char_to_int[char]] = 1.\n",
        "\n",
        "\n",
        "  decoder_input = np.zeros((len(data_input),max_output_len,output_alphabet_len), dtype=np.float)\n",
        "  decoder_output = np.zeros((len(data_input),max_output_len,output_alphabet_len), dtype=np.float)\n",
        "\n",
        "  for idx_s, output_sentence in enumerate(data_output):        \n",
        "      for idx_c, char in enumerate(output_sentence):\n",
        "          decoder_input[idx_s][idx_c][output_alphabet_from_char_to_int[char]] = 1.\n",
        "          if idx_c > 0:\n",
        "              decoder_output[idx_s][idx_c-1][output_alphabet_from_char_to_int[char]] = 1.\n",
        "              \n",
        "  return encoder_input, decoder_input, decoder_output\n",
        "\n",
        "# Función generadora\n",
        "def generator(data_input_full, data_output_full, batch_size=16):  \n",
        "  while True:\n",
        "    for idx in range(0,len(data_input_full), batch_size):\n",
        "      data_input = data_input_full[idx:idx+batch_size]\n",
        "      data_output = data_output_full[idx:idx+batch_size]\n",
        "                  \n",
        "      x,y,t = batch_to_vectors(data_input,data_output)\n",
        "      yield [x,y], t\n",
        "    \n",
        "    \n",
        "# Inicializamos un generador para la comprobación\n",
        "check_generator = generator(data_input, data_output)\n",
        "\n",
        "for _ in range(5):\n",
        "  [encoder_input, decoder_input], decoder_output = next(check_generator)\n",
        "  print ('')\n",
        "  print (encoder_input.shape)\n",
        "  print (decoder_input.shape)\n",
        "  print (decoder_output.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(16, 13, 65)\n",
            "(16, 20, 77)\n",
            "(16, 20, 77)\n",
            "\n",
            "(16, 14, 65)\n",
            "(16, 26, 77)\n",
            "(16, 26, 77)\n",
            "\n",
            "(16, 13, 65)\n",
            "(16, 22, 77)\n",
            "(16, 22, 77)\n",
            "\n",
            "(16, 14, 65)\n",
            "(16, 24, 77)\n",
            "(16, 24, 77)\n",
            "\n",
            "(16, 13, 65)\n",
            "(16, 24, 77)\n",
            "(16, 24, 77)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2iYDNV3edBh",
        "colab_type": "text"
      },
      "source": [
        "Ahora vamos a construir el model neuronal. Cosas a tener en cuenta:\n",
        "*  ```return_sequences=True``` hace que nos devuelva las predicciones en cada paso.\n",
        "* ``` return_states=True``` hace que se devuelvan los estados internos.\n",
        "* Como vamos a utilizar celdas LSTM, los estados internos se componen tanto del estado interno en sí (h) como de la celda memoria (c)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iShWBS13ehPB",
        "colab_type": "code",
        "outputId": "41d79273-00a3-44e3-9fa1-73eb768c4262",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(None, input_alphabet_len), name='encoder_input')\n",
        "encoder_outputs, encoder_h, encoder_c = LSTM(64, return_state=True, name='encoder_output')(encoder_inputs)\n",
        "context_vector = [encoder_h, encoder_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(None, output_alphabet_len), name='decoder_input')\n",
        "decoder_outputs, _, _ = LSTM(64, return_sequences=True, return_state=True, name='decoder_lstm')(decoder_inputs, initial_state=context_vector)\n",
        "decoder_outputs = Dense(output_alphabet_len, activation='softmax',name='decoder_output')(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')   \n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_input (InputLayer)      (None, None, 65)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "decoder_input (InputLayer)      (None, None, 77)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_output (LSTM)           [(None, 64), (None,  33280       encoder_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "decoder_lstm (LSTM)             [(None, None, 64), ( 36352       decoder_input[0][0]              \n",
            "                                                                 encoder_output[0][1]             \n",
            "                                                                 encoder_output[0][2]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder_output (Dense)          (None, None, 77)     5005        decoder_lstm[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 74,637\n",
            "Trainable params: 74,637\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhnjH-8RgeD9",
        "colab_type": "text"
      },
      "source": [
        "## Fase de entrenamiento\n",
        "\n",
        "Vamos a utilizar 1 % del corpus para validar, dejando el 99 % para entrenar la red.\n",
        "\n",
        "Para la función de entrenamiento, creamos un generador para el entrenamiento y otro para la validación. No obstante, también obtendremos el paquete completo de validación para monitorizar visualmente qué ocurre."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRoG_z1rtpQL",
        "colab_type": "code",
        "outputId": "cbdf5944-638a-4f67-8597-3e6d46b14fa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "val_split = 0.01\n",
        "idx_split = int(len(data_input)*val_split)\n",
        "\n",
        "data_input_train = data_input[idx_split:]\n",
        "data_output_train = data_output[idx_split:]\n",
        "\n",
        "data_input_validation = data_input[:idx_split]\n",
        "data_output_validation = data_output[:idx_split]\n",
        "\n",
        "training_generator = generator(data_input_train, data_output_train)\n",
        "validation_generator = generator(data_input_validation, data_output_validation)\n",
        "\n",
        "x_val, y_val, t_val = batch_to_vectors(data_input_validation,data_output_validation)\n",
        "\n",
        "print('Tamaño del conjunto de validación: ' + str(x_val.shape[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tamaño del conjunto de validación: 40\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDThbVMttoCm",
        "colab_type": "text"
      },
      "source": [
        "Entrenamos con los generadores, para lo cual hay que especificar cuántos pasos corresponden a una época: número de datos dividido por el tamaño de los paquetes de datos. Tras cada época, vamos a comprobar qué resultados de traducción proporciona la red."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PTzl8lIg-Ui",
        "colab_type": "code",
        "outputId": "c95846cb-ec88-4215-f013-ef2101ff31ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3927
        }
      },
      "source": [
        "# 15 épocas \n",
        "for epoch in range(15):\n",
        "    print('Epoca ' + str(epoch))\n",
        "  \n",
        "    # Entrenamos durante una época\n",
        "    model.fit_generator(training_generator, \n",
        "                       steps_per_epoch=len(data_input_train)//16, \n",
        "                       verbose=2, \n",
        "                       epochs=1, \n",
        "                       validation_data=[[x_val,y_val],t_val])\n",
        "    \n",
        "    # Predecimos sobre el paquete de validación\n",
        "    batch_prediction = model.predict([x_val,y_val],batch_size=16)\n",
        "    \n",
        "    # Comprobamos la traduccion de las 5 primeras secuencias\n",
        "    for idx,sentence_prediction in enumerate(batch_prediction[:5]):  \n",
        "        \n",
        "        # Obtenemos la frase predicha\n",
        "        raw_predicted_sequence = [output_alphabet_from_int_to_char[char] for char in np.argmax(sentence_prediction,axis=1)]\n",
        "        \n",
        "        # Leemos solo hasta el caracter EOS\n",
        "        predicted_sentence = output_sos         \n",
        "        for char in raw_predicted_sequence:\n",
        "            predicted_sentence += char\n",
        "            if char == output_eos:\n",
        "                break                    \n",
        "        \n",
        "        # Resultado\n",
        "        print( 'Entrada:\\t' + str(data_input_validation[idx]))\n",
        "        print( 'Predicción:\\t' + str(predicted_sentence) )\n",
        "        print( 'Sal. esperada:\\t' + str(data_output_validation[idx]) )\n",
        "        print('')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoca 0\n",
            "Epoch 1/1\n",
            " - 9s - loss: 1.9131 - val_loss: 1.6868\n",
            "Entrada:\tTake mine.\n",
            "Predicción:\t<Eo      eeo.>\n",
            "Sal. esperada:\t<Toma el mío.>\n",
            "\n",
            "Entrada:\tHi.\n",
            "Predicción:\t<Eo      o>\n",
            "Sal. esperada:\t<Hola.>\n",
            "\n",
            "Entrada:\tCount on it.\n",
            "Predicción:\t<Eoe     e  o...>\n",
            "Sal. esperada:\t<Cuente con eso.>\n",
            "\n",
            "Entrada:\tHe's in pain.\n",
            "Predicción:\t<Eo      eee .o.>\n",
            "Sal. esperada:\t<Está sufriendo.>\n",
            "\n",
            "Entrada:\tI am a man.\n",
            "Predicción:\t<Eo      ee o....>\n",
            "Sal. esperada:\t<Soy una persona.>\n",
            "\n",
            "Epoca 1\n",
            "Epoch 1/1\n",
            " - 8s - loss: 1.6173 - val_loss: 1.4147\n",
            "Entrada:\tTake mine.\n",
            "Predicción:\t<Eo   esoee..>\n",
            "Sal. esperada:\t<Toma el mío.>\n",
            "\n",
            "Entrada:\tHi.\n",
            "Predicción:\t<Ee   >\n",
            "Sal. esperada:\t<Hola.>\n",
            "\n",
            "Entrada:\tCount on it.\n",
            "Predicción:\t<Eoe  e ee  est.>\n",
            "Sal. esperada:\t<Cuente con eso.>\n",
            "\n",
            "Entrada:\tHe's in pain.\n",
            "Predicción:\t<Es e e eeee..o.>\n",
            "Sal. esperada:\t<Está sufriendo.>\n",
            "\n",
            "Entrada:\tI am a man.\n",
            "Predicción:\t<Eo  ee  eera....>\n",
            "Sal. esperada:\t<Soy una persona.>\n",
            "\n",
            "Epoca 2\n",
            "Epoch 1/1\n",
            " - 8s - loss: 1.4012 - val_loss: 1.2622\n",
            "Entrada:\tTake mine.\n",
            "Predicción:\t<Eom  esoeer.>\n",
            "Sal. esperada:\t<Toma el mío.>\n",
            "\n",
            "Entrada:\tHi.\n",
            "Predicción:\t<¡am  >\n",
            "Sal. esperada:\t<Hola.>\n",
            "\n",
            "Entrada:\tCount on it.\n",
            "Predicción:\t<Eoen a eastest.>\n",
            "Sal. esperada:\t<Cuente con eso.>\n",
            "\n",
            "Entrada:\tHe's in pain.\n",
            "Predicción:\t<Estoseeeeeento.>\n",
            "Sal. esperada:\t<Está sufriendo.>\n",
            "\n",
            "Entrada:\tI am a man.\n",
            "Predicción:\t<Tom ee  eera....>\n",
            "Sal. esperada:\t<Soy una persona.>\n",
            "\n",
            "Epoca 3\n",
            "Epoch 1/1\n",
            " - 8s - loss: 1.2861 - val_loss: 1.1918\n",
            "Entrada:\tTake mine.\n",
            "Predicción:\t<Eom  es aa..>\n",
            "Sal. esperada:\t<Toma el mío.>\n",
            "\n",
            "Entrada:\tHi.\n",
            "Predicción:\t<Ma   >\n",
            "Sal. esperada:\t<Hola.>\n",
            "\n",
            "Entrada:\tCount on it.\n",
            "Predicción:\t<Moen a ea  ast.>\n",
            "Sal. esperada:\t<Cuente con eso.>\n",
            "\n",
            "Entrada:\tHe's in pain.\n",
            "Predicción:\t<Esto eoeeaen.o.>\n",
            "Sal. esperada:\t<Está sufriendo.>\n",
            "\n",
            "Entrada:\tI am a man.\n",
            "Predicción:\t<To  ee  aara....>\n",
            "Sal. esperada:\t<Soy una persona.>\n",
            "\n",
            "Epoca 4\n",
            "Epoch 1/1\n",
            " - 8s - loss: 1.2192 - val_loss: 1.1497\n",
            "Entrada:\tTake mine.\n",
            "Predicción:\t<Mome es aa..>\n",
            "Sal. esperada:\t<Toma el mío.>\n",
            "\n",
            "Entrada:\tHi.\n",
            "Predicción:\t<¡a   >\n",
            "Sal. esperada:\t<Hola.>\n",
            "\n",
            "Entrada:\tCount on it.\n",
            "Predicción:\t<¿oenta eo  ast.>\n",
            "Sal. esperada:\t<Cuente con eso.>\n",
            "\n",
            "Entrada:\tHe's in pain.\n",
            "Predicción:\t<Nsto eoeeaento.>\n",
            "Sal. esperada:\t<Está sufriendo.>\n",
            "\n",
            "Entrada:\tI am a man.\n",
            "Predicción:\t<Eoy eet aaraa...>\n",
            "Sal. esperada:\t<Soy una persona.>\n",
            "\n",
            "Epoca 5\n",
            "Epoch 1/1\n",
            " - 8s - loss: 1.1722 - val_loss: 1.0932\n",
            "Entrada:\tTake mine.\n",
            "Predicción:\t<Some es co..>\n",
            "Sal. esperada:\t<Toma el mío.>\n",
            "\n",
            "Entrada:\tHi.\n",
            "Predicción:\t<¡ay  >\n",
            "Sal. esperada:\t<Hola.>\n",
            "\n",
            "Entrada:\tCount on it.\n",
            "Predicción:\t<¿oento eontcnt.>\n",
            "Sal. esperada:\t<Cuente con eso.>\n",
            "\n",
            "Entrada:\tHe's in pain.\n",
            "Predicción:\t<Nsto eoeeaento.>\n",
            "Sal. esperada:\t<Está sufriendo.>\n",
            "\n",
            "Entrada:\tI am a man.\n",
            "Predicción:\t<Eoy ent aarao...>\n",
            "Sal. esperada:\t<Soy una persona.>\n",
            "\n",
            "Epoca 6\n",
            "Epoch 1/1\n",
            " - 8s - loss: 1.1334 - val_loss: 1.0702\n",
            "Entrada:\tTake mine.\n",
            "Predicción:\t<Meme as ao..>\n",
            "Sal. esperada:\t<Toma el mío.>\n",
            "\n",
            "Entrada:\tHi.\n",
            "Predicción:\t<¡ay  >\n",
            "Sal. esperada:\t<Hola.>\n",
            "\n",
            "Entrada:\tCount on it.\n",
            "Predicción:\t<¡oenta aon ant.>\n",
            "Sal. esperada:\t<Cuente con eso.>\n",
            "\n",
            "Entrada:\tHe's in pain.\n",
            "Predicción:\t<Nsto eoeeae..o.>\n",
            "Sal. esperada:\t<Está sufriendo.>\n",
            "\n",
            "Entrada:\tI am a man.\n",
            "Predicción:\t<Eoy ent aarao...>\n",
            "Sal. esperada:\t<Soy una persona.>\n",
            "\n",
            "Epoca 7\n",
            "Epoch 1/1\n",
            " - 8s - loss: 1.1014 - val_loss: 1.0382\n",
            "Entrada:\tTake mine.\n",
            "Predicción:\t<Moma es ae..>\n",
            "Sal. esperada:\t<Toma el mío.>\n",
            "\n",
            "Entrada:\tHi.\n",
            "Predicción:\t<¡a   >\n",
            "Sal. esperada:\t<Hola.>\n",
            "\n",
            "Entrada:\tCount on it.\n",
            "Predicción:\t<¡oente eontant.>\n",
            "Sal. esperada:\t<Cuente con eso.>\n",
            "\n",
            "Entrada:\tHe's in pain.\n",
            "Predicción:\t<Esto eoeeaen.o.>\n",
            "Sal. esperada:\t<Está sufriendo.>\n",
            "\n",
            "Entrada:\tI am a man.\n",
            "Predicción:\t<Eoy en  aaraa...>\n",
            "Sal. esperada:\t<Soy una persona.>\n",
            "\n",
            "Epoca 8\n",
            "Epoch 1/1\n",
            " - 8s - loss: 1.0725 - val_loss: 1.0183\n",
            "Entrada:\tTake mine.\n",
            "Predicción:\t<Moma as me..>\n",
            "Sal. esperada:\t<Toma el mío.>\n",
            "\n",
            "Entrada:\tHi.\n",
            "Predicción:\t<¡a   >\n",
            "Sal. esperada:\t<Hola.>\n",
            "\n",
            "Entrada:\tCount on it.\n",
            "Predicción:\t<¡oenta aon ant.>\n",
            "Sal. esperada:\t<Cuente con eso.>\n",
            "\n",
            "Entrada:\tHe's in pain.\n",
            "Predicción:\t<Nsto eoeeaen.o.>\n",
            "Sal. esperada:\t<Está sufriendo.>\n",
            "\n",
            "Entrada:\tI am a man.\n",
            "Predicción:\t<Eoy en  aaraa..d>\n",
            "Sal. esperada:\t<Soy una persona.>\n",
            "\n",
            "Epoca 9\n",
            "Epoch 1/1\n",
            " - 8s - loss: 1.0472 - val_loss: 0.9991\n",
            "Entrada:\tTake mine.\n",
            "Predicción:\t<Doma as me..>\n",
            "Sal. esperada:\t<Toma el mío.>\n",
            "\n",
            "Entrada:\tHi.\n",
            "Predicción:\t<¡a   >\n",
            "Sal. esperada:\t<Hola.>\n",
            "\n",
            "Entrada:\tCount on it.\n",
            "Predicción:\t<¡oenta aontant.>\n",
            "Sal. esperada:\t<Cuente con eso.>\n",
            "\n",
            "Entrada:\tHe's in pain.\n",
            "Predicción:\t<Está soeeaen.o.>\n",
            "Sal. esperada:\t<Está sufriendo.>\n",
            "\n",
            "Entrada:\tI am a man.\n",
            "Predicción:\t<Soy mn  darda..d>\n",
            "Sal. esperada:\t<Soy una persona.>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGlnY_zKxSbe",
        "colab_type": "text"
      },
      "source": [
        "## Fase de predicción\n",
        "\n",
        "Una vez el modelo está entrenado, podemos usarlo para predecir frases que no se han utilizado en ningún momento. La diferencia con el caso de predecir sobre el conjunto de validación es que no podemos disponer a priori de las entradas del decoder. A cambio, tenemos que hacer una predicción* en linea*, en la cual predecimos simplemente el siguiente símbolo dado el estado actual del decoder y el último símbolo predicho.\n",
        "\n",
        "Para esto, debemos acceder a las capas internas del modelo creado. Esta funcionalidad se puede realizar en Keras, ya que se puede acceder a las capas de un modelo a partir de su nombre."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw0djlNACCtC",
        "colab_type": "code",
        "outputId": "18c08098-6e2c-4e56-81d2-b992ecc8e9f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "for layer in model.layers:\n",
        "  print (layer.name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder_input\n",
            "decoder_input\n",
            "encoder_output\n",
            "decoder_lstm\n",
            "decoder_output\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXzH0AXGCCDa",
        "colab_type": "text"
      },
      "source": [
        "Para predecir una frase, necesitamos obtener el context vector. Para ello, necesitamos un modelo que acepte una entrada y nos proporcione el último estado del encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBfqeYtY0C_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creamos el modelo encoder a partir de las capas del modelo entrenado\n",
        "encoder_model = Model(model.get_layer(\"encoder_input\").input, model.get_layer(\"encoder_output\").output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtsJc3E1DllD",
        "colab_type": "text"
      },
      "source": [
        " Por otra parte, necesitamos un modelo que acepte un context vector y un elemento, y nos prediga el siguiente elemento de la secuencia a la vez que devuelve el siguiente context vector. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5esLABUFDt3u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creamos las entradas para el context vector inicial \n",
        "decoder_state_input_h = Input(shape=(64,))\n",
        "decoder_state_input_c = Input(shape=(64,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "# Obtenemos las referencias a los componentes del decoder\n",
        "decoder_inputs = model.get_layer(\"decoder_input\").input\n",
        "decoder_lstm_output, state_h, state_c = model.get_layer(\"decoder_lstm\")(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = model.get_layer(\"decoder_output\")(decoder_lstm_output)\n",
        "\n",
        "# Creamos un modelo, a partir de las capas del modelo entrenado, que \n",
        "# - recibe una secuencia (la última predicción) y un vector de contexto\n",
        "# - proporciona una predicción y los siguientes estados\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7y9IFzMFIDh",
        "colab_type": "text"
      },
      "source": [
        "Con los dos pseudo-modelos creados anteriormente, podemos proceder a predecir una secuencia por completo. Los pasos básicos son:\n",
        "* Obtener el context vector del encoder\n",
        "* Predecir los siguientes elementos uno a uno, actualizando el context vector.\n",
        "\n",
        "Para ello, vamos a definir una función que recibe una frase y realiza la traducción."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8M6lOerQ1U8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Función que recibe una frase en inglés\n",
        "def translate_sequence(input_sentence):\n",
        "  # Creamos el paquete de entrada al decoder, que será una única secuencia de longitud igual al número de caracteres\n",
        "  input_encoder_seq = np.zeros((1,len(input_sentence),input_alphabet_len), dtype=np.float)\n",
        "  \n",
        "  # Convertimos la secuencia a forma codificada one-hot\n",
        "  for idx_c, char in enumerate(input_sentence):\n",
        "          input_encoder_seq[0][idx_c][input_alphabet_from_char_to_int[char]] = 1.\n",
        "  \n",
        "  # Utilizamos el pseudo-modelo encoder para obtener el context vector\n",
        "  _, h, c = encoder_model.predict(input_encoder_seq)\n",
        "  \n",
        "  # Creamos el context vector concatenando h y c\n",
        "  states_value = [h,c]\n",
        "\n",
        "  # Creamos la secuencia de entrada al decoder: sólo necesitamos un elemento\n",
        "  input_decoder_seq = np.zeros((1, 1, output_alphabet_len))\n",
        "  # Codificamos el elemento SOS como one-hot\n",
        "  input_decoder_seq[0, 0, output_alphabet_from_char_to_int[output_sos]] = 1.\n",
        "\n",
        "  \n",
        "  # Bucle infinito para crear la secuencia predicha\n",
        "  decoded_sentence = ''\n",
        "  while True:\n",
        "      # Predicción probabilista y siguiente estado\n",
        "      softmax_prediction, h, c = decoder_model.predict([input_decoder_seq] + states_value)\n",
        "\n",
        "      # Elegimos el elemento con la mayor probabilidad\n",
        "      prediction = np.argmax(softmax_prediction[0],axis=1)[0]\n",
        "      \n",
        "      # Lo convertimos a caracter\n",
        "      predicter_character = output_alphabet_from_int_to_char[prediction]\n",
        "\n",
        "      # Criterio de parada: encontrar el EOS (o tener una secuencia demasiado larga)\n",
        "      if (predicter_character == output_eos or len(decoded_sentence) > 30):\n",
        "          break\n",
        "\n",
        "      # Concatenamos el caracter\n",
        "      decoded_sentence += predicter_character\n",
        "\n",
        "      # Actualizamos el context vector para la siguiente iteración       \n",
        "      states_value = [h, c]\n",
        "      \n",
        "      # Actualizamos la entrada del decoder para la siguiente iteración\n",
        "      input_decoder_seq = np.zeros((1, 1, output_alphabet_len))\n",
        "      input_decoder_seq[0, 0, output_alphabet_from_char_to_int[predicter_character]] = 1.\n",
        "\n",
        "  return decoded_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pViZ1PQL4EJl",
        "colab_type": "code",
        "outputId": "86a5c492-5c94-4a3e-b7c8-059165f4fe8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "  \n",
        "print(translate_sequence('This is.'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Está esto.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeh8yLbNxpBj",
        "colab_type": "text"
      },
      "source": [
        "La traducción entre lenguajes complejos como el inglés y el español es dificil de modelar. Los resultados obtenidos en este ejemplo están lejos de la fiabilidad que alcanzan los esquemas actuales. No obstante, la formulación de los modelos del estado del arte son similares: la diferencia rádica especialmente en la capacidad de cómputo y la cantidad de datos. No obstante, existen otras cuestiones que hemos obviado en este ejercicio como la búsqueda de hiper-parámetros adecuados, modelos de lenguaje, (word) embeddings o modelos de atención."
      ]
    }
  ]
}